{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/lucky/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import itertools\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "sent_tag = brown.tagged_sents()\n",
    "mod_sent_tag=[]\n",
    "for s in sent_tag:\n",
    "  s.insert(0,('##','##'))\n",
    "  s.append(('&&','&&'))\n",
    "  mod_sent_tag.append(s)\n",
    "\n",
    "split_num = int(len(mod_sent_tag)*0.9)\n",
    "train_data = mod_sent_tag[0:split_num]\n",
    "test_data = mod_sent_tag[split_num:]\n",
    "\n",
    "train_word_tag = {}\n",
    "for s in train_data:\n",
    "  for (w,t) in s:\n",
    "    w=w.lower()\n",
    "    try:\n",
    "      try:\n",
    "        train_word_tag[t][w]+=1\n",
    "      except:\n",
    "        train_word_tag[t][w]=1\n",
    "    except:\n",
    "      train_word_tag[t]={w:1}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "train_emission_prob={}\n",
    "for k in train_word_tag.keys():\n",
    "  train_emission_prob[k]={}\n",
    "  count = sum(train_word_tag[k].values())\n",
    "  for k2 in train_word_tag[k].keys():\n",
    "    train_emission_prob[k][k2]=train_word_tag[k][k2]/count\n",
    "\n",
    "bigram_tag_data = {}\n",
    "for s in train_data:\n",
    "  bi=list(nltk.bigrams(s))\n",
    "  for b1,b2 in bi:\n",
    "    try:\n",
    "      try:\n",
    "        bigram_tag_data[b1[1]][b2[1]]+=1\n",
    "      except:\n",
    "        bigram_tag_data[b1[1]][b2[1]]=1\n",
    "    except:\n",
    "      bigram_tag_data[b1[1]]={b2[1]:1}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "bigram_tag_prob={}\n",
    "for k in bigram_tag_data.keys():\n",
    "  bigram_tag_prob[k]={}\n",
    "  count=sum(bigram_tag_data[k].values())\n",
    "  for k2 in bigram_tag_data[k].keys():\n",
    "    bigram_tag_prob[k][k2]=bigram_tag_data[k][k2]/count\n",
    "\n",
    "tags_of_tokens = {}\n",
    "count=0\n",
    "for s in train_data:\n",
    "  for (w,t) in s:\n",
    "    w=w.lower()\n",
    "    try:\n",
    "      if t not in tags_of_tokens[w]:\n",
    "        tags_of_tokens[w].append(t)\n",
    "    except:\n",
    "      l = []\n",
    "      l.append(t)\n",
    "      tags_of_tokens[w] = l\n",
    "\n",
    "for s in test_data:\n",
    "  for (w,t) in s:\n",
    "    w=w.lower()\n",
    "    try:\n",
    "      if t not in tags_of_tokens[w]:\n",
    "        tags_of_tokens[w].append(t)\n",
    "    except:\n",
    "      l = []\n",
    "      l.append(t)\n",
    "      tags_of_tokens[w] = l\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "test_words=[]\n",
    "test_tags=[]\n",
    "for s in test_data:\n",
    "  temp_word=[]\n",
    "  temp_tag=[]\n",
    "  for (w,t) in s:\n",
    "    temp_word.append(w.lower())\n",
    "    temp_tag.append(t)\n",
    "  test_words.append(temp_word)\n",
    "  test_tags.append(temp_tag)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "#Executing the Viterbi Algorithm\n",
    "predicted_tags = []                #intializing the predicted tags\n",
    "for x in range(len(test_words)):   # for each tokenized sentence in the test data\n",
    "  s = test_words[x]\n",
    "  #storing_values is a dictionary which stores the required values\n",
    "  #ex: storing_values = {step_no.:{state1:[previous_best_state,value_of_the_state]}}\n",
    "  storing_values = {}\n",
    "  for q in range(len(s)):\n",
    "    step = s[q]\n",
    "    #for the starting word of the sentence\n",
    "    if q == 1:\n",
    "      storing_values[q] = {}\n",
    "      tags = tags_of_tokens[step]\n",
    "      for t in tags:\n",
    "        #this is applied since we do not know whether the word in the test data is present in train data or not\n",
    "        try:\n",
    "          storing_values[q][t] = ['##',bigram_tag_prob['##'][t]*train_emission_prob[t][step]]\n",
    "        #if word is not present in the train data but present in test data we assign a very low probability of 0.0001\n",
    "        except:\n",
    "          storing_values[q][t] = ['##',0.0001]#*train_emission_prob[t][step]]\n",
    "\n",
    "    #if the word is not at the start of the sentence\n",
    "    if q>1:\n",
    "      storing_values[q] = {}\n",
    "      previous_states = list(storing_values[q-1].keys())   # loading the previous states\n",
    "      current_states  = tags_of_tokens[step]               # loading the current states\n",
    "      #calculation of the best previous state for each current state and then storing\n",
    "      #it in storing_values\n",
    "      for t in current_states:\n",
    "        temp = []\n",
    "        for pt in previous_states:\n",
    "          try:\n",
    "            temp.append(storing_values[q-1][pt][1]*bigram_tag_prob[pt][t]*train_emission_prob[t][step])\n",
    "          except:\n",
    "            temp.append(storing_values[q-1][pt][1]*0.0001)\n",
    "        max_temp_index = temp.index(max(temp))\n",
    "        best_pt = previous_states[max_temp_index]\n",
    "        storing_values[q][t]=[best_pt,max(temp)]\n",
    "\n",
    "\n",
    "  #Backtracing to extract the best possible tags for the sentence\n",
    "  pred_tags = []\n",
    "  total_steps_num = storing_values.keys()\n",
    "  last_step_num = max(total_steps_num)\n",
    "  for bs in range(len(total_steps_num)):\n",
    "    step_num = last_step_num - bs\n",
    "    if step_num == last_step_num:\n",
    "      pred_tags.append('&&')\n",
    "      pred_tags.append(storing_values[step_num]['&&'][0])\n",
    "    if step_num<last_step_num and step_num>0:\n",
    "      pred_tags.append(storing_values[step_num][pred_tags[len(pred_tags)-1]][0])\n",
    "  predicted_tags.append(list(reversed(pred_tags)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "right = 0\n",
    "wrong = 0\n",
    "for i in range(len(test_tags)):\n",
    "  gt = test_tags[i]\n",
    "  pred = predicted_tags[i]\n",
    "  for h in range(len(gt)):\n",
    "    if gt[h] == pred[h]:\n",
    "      right = right+1\n",
    "    else:\n",
    "      wrong = wrong +1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score test data is:  0.9213450565896548\n"
     ]
    }
   ],
   "source": [
    "print('Precision score test data is: ',right/(right+wrong))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}